{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "598f0aad-4fd3-442d-bbeb-6e9f5856bb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How to handle corrupt records and read it in different modes \n",
    "\n",
    "'''\n",
    "id,name,age,salary,address,nominee\n",
    "1,Manish,26,75000,bihar,nominee1\n",
    "2,Nikita,23,100000,uttarpradesh,nominee2\n",
    "3,Pritam,22,150000,Bangalore,India,nominee3\n",
    "4,Prantosh,17,200000,Kolkata,India,nominee4\n",
    "5,Vikash,31,300000,,nominee5\n",
    "\n",
    "This is how my data looks like look at id 3,4 they are corrupted now below we will see how can we handle it in diff ways\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9818c699-6330-40ad-a761-b61b0e7f2166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|   India|\n|  4|Prantosh| 17|200000|     Kolkata|   India|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# When used permissivive mode\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferschema\",\"true\")\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "            .load(\"/FileStore/tables/csv.txt\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c881536a-2a22-4e25-839e-db1665fba3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+\n| id|  name|age|salary|     address| nominee|\n+---+------+---+------+------------+--------+\n|  1|Manish| 26| 75000|       bihar|nominee1|\n|  2|Nikita| 23|100000|uttarpradesh|nominee2|\n|  5|Vikash| 31|300000|        null|nominee5|\n+---+------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# when used in dropmalformed not take into consideration the corrupt data so those data will be excluded\n",
    "\n",
    "df1 = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferschema\",\"true\")\\\n",
    "            .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "            .load(\"/FileStore/tables/csv.txt\")\n",
    "\n",
    "df1.show()\n",
    "\n",
    "# as you can see only 3 rows the 2 corrupted rows not considered In FAILFAST you wont even get this and directly error will be thrown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a28a6f0-39d4-4b1e-b6bd-71af246a4a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|   India|\n|  4|Prantosh| 17|200000|     Kolkata|   India|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361cd18c-19d4-496f-b8fd-217c18f569a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n|id |name    |age|salary|address     |nominee |_corrupt_record                            |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n|1  |Manish  |26 |75000 |bihar       |nominee1|null                                       |\n|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null                                       |\n|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n|5  |Vikash  |31 |300000|null        |nominee5|null                                       |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# How can we print corrupted records \n",
    "\n",
    "# We will use permissive mode and in that there is a feature where in you can store corrupted data below is the process on how you do it \n",
    "\n",
    "# First create a schema use that schema and read file so whenever there is records in corrupted coloumn that means that row has corrupted records\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "                        StructField(\"id\",IntegerType(),True),\n",
    "                        StructField(\"name\",StringType(),True),\n",
    "                        StructField(\"age\",IntegerType(),True),\n",
    "                        StructField(\"salary\",IntegerType(),True),\n",
    "                        StructField(\"address\",StringType(),True),\n",
    "                        StructField(\"nominee\",StringType(),True),\n",
    "                        StructField(\"_corrupt_record\",StringType(),True) # name should be this different name will not give full record but will just put the record which is next\n",
    "                            ])\n",
    "\n",
    "df2 = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferschema\",\"true\")\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "            .schema(schema)\\\n",
    "            .load(\"/FileStore/tables/csv.txt\")\n",
    "\n",
    "df2.show(truncate= False)  # Truncate false will not cut data and show full value\n",
    "\n",
    "# here you can see 3, 4th row has corrupted coloumn filled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481a08d2-f432-4ec4-a5cb-26b95ff959d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+---------------+\n| id|  name|age|salary|     address| nominee|_corrupt_record|\n+---+------+---+------+------------+--------+---------------+\n|  1|Manish| 26| 75000|       bihar|nominee1|           null|\n|  2|Nikita| 23|100000|uttarpradesh|nominee2|           null|\n|  5|Vikash| 31|300000|        null|nominee5|           null|\n+---+------+---+------+------------+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# How can we store corrupt data in some other file for reference to maybe later clean it \n",
    "\n",
    "# we use a option in spark while readin from csv so what it does is whenever it detects corrupt data in _coorupt_record coloumn it sends that data into another file storage that we specified we can then delete this data from our main df for further analysis and get back to this data later \n",
    "\n",
    "df2 = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferschema\",\"true\")\\\n",
    "            .schema(schema)\\\n",
    "            .option(\"badRecordsPath\",\"/FileStore/tables/bad_records\")\\\n",
    "            .load(\"/FileStore/tables/csv.txt\")\n",
    "\n",
    "# so here when we use option of badrecords 1st we need schema and sceondly we do not need which mode to run on since badrecord has its own default mode set to work \n",
    "\n",
    "# This badrecords are default stored in json format \n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5679eee2-f3b5-42f4-80cc-383cafc48d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2010_summary.csv</td><td>2010_summary.csv</td><td>7121</td><td>1741277334000</td></tr><tr><td>dbfs:/FileStore/tables/bad_records/</td><td>bad_records/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/csv.txt</td><td>csv.txt</td><td>230</td><td>1741448093000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2010_summary.csv",
         "2010_summary.csv",
         7121,
         1741277334000
        ],
        [
         "dbfs:/FileStore/tables/bad_records/",
         "bad_records/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/csv.txt",
         "csv.txt",
         230,
         1741448093000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aeb3fa3-e3de-4fc7-bec1-2856215d7cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/bad_records/20250308T161530/bad_records/part-00000-787ea353-c81e-474d-b4b5-56fab9ca38a1</td><td>part-00000-787ea353-c81e-474d-b4b5-56fab9ca38a1</td><td>474</td><td>1741450532000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/bad_records/20250308T161530/bad_records/part-00000-787ea353-c81e-474d-b4b5-56fab9ca38a1",
         "part-00000-787ea353-c81e-474d-b4b5-56fab9ca38a1",
         474,
         1741450532000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy path from catlog in databricks where your files are present\n",
    "%fs\n",
    "ls /FileStore/tables/bad_records/20250308T161530/bad_records/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96893bfc-fa1a-45c9-aa9a-69350facef27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n|                path|              reason|              record|\n+--------------------+--------------------+--------------------+\n|dbfs:/FileStore/t...|org.apache.spark....|3,Pritam,22,15000...|\n|dbfs:/FileStore/t...|org.apache.spark....|4,Prantosh,17,200...|\n+--------------------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# How can we read this bad data\n",
    "\n",
    "bad_df = spark.read.format(\"json\").load(\"/FileStore/tables/bad_records/20250308T161530/bad_records/part-00000-787ea353-c81e-474d-b4b5-56fab9ca38a1\")\n",
    "\n",
    "bad_df.show()\n",
    "\n",
    "#This is how we can store and see bad records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ccd5500-30e1-404b-b07e-c7e6cc66bcf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1628609559472962,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_2()corrupt data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
