{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5dadb5-7b03-4e5c-ad19-423441ff899d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=606795570777477#setting/sparkui/0310-042716-57xxjif5/driver-7100639363616082717\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=606795570777477#setting/sparkui/0310-042716-57xxjif5/driver-7100639363616082717\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcaba88d-2e0c-4fe7-96b7-d98aad3fdecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark transformation basically has 2 methods Dataframe APi or using spark sql but we wont do spark sql since we know normal sql very well, we will try to understand how to do transformation and important transformations in dataframe api\n",
    "\n",
    "\n",
    "my_data = [(1,1),\n",
    "        (2,1),\n",
    "        (3,1),\n",
    "        (4,2),\n",
    "        (5,1),\n",
    "        (6,2),\n",
    "        (7,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1359d7a-3b1f-464d-973a-8fe84a2b03d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * # for future when needed \n",
    "\n",
    "my_schema = ['id', 'num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5940ccd8-3488-4876-b36e-e31d280cbb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=my_data, schema=my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd062ca5-2c2f-457e-a879-4156a22641e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  1|\n|  1|\n|  1|\n|  2|\n|  1|\n|  2|\n|  2|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"num\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1410c1-30cd-404d-9599-1bfa57bfaa92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformation Starts From below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97aeb73-bbf4-4d14-bfe1-a9c42bcf01e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EMPLOYEE_ID: integer (nullable = true)\n |-- FIRST_NAME: string (nullable = true)\n |-- LAST_NAME: string (nullable = true)\n |-- EMAIL: string (nullable = true)\n |-- PHONE_NUMBER: string (nullable = true)\n |-- HIRE_DATE: string (nullable = true)\n |-- JOB_ID: string (nullable = true)\n |-- SALARY: integer (nullable = true)\n |-- COMMISSION_PCT: string (nullable = true)\n |-- MANAGER_ID: string (nullable = true)\n |-- DEPARTMENT_ID: integer (nullable = true)\n\n+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|  JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|SH_CLERK|  2600|            - |       124|           50|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03| AD_ASST|  4400|            - |       101|           10|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|  MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|  MK_REP|  6000|            - |       201|           20|\n+-----------+----------+---------+--------+------------+---------+--------+------+--------------+----------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .option(\"inferschema\",\"true\")\\\n",
    "                .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/FileStore/tables/to_transform_csv.txt\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f244a77-f410-48ba-8e02-1e71ff565606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: ['EMPLOYEE_ID',\n 'FIRST_NAME',\n 'LAST_NAME',\n 'EMAIL',\n 'PHONE_NUMBER',\n 'HIRE_DATE',\n 'JOB_ID',\n 'SALARY',\n 'MANAGER_ID',\n 'DEPARTMENT_ID']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99470de4-99cf-400c-a43d-2a6eefe2c10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EMPLOYEE_ID: integer (nullable = true)\n |-- FIRST_NAME: string (nullable = true)\n |-- LAST_NAME: string (nullable = true)\n |-- EMAIL: string (nullable = true)\n |-- PHONE_NUMBER: string (nullable = true)\n |-- HIRE_DATE: date (nullable = true)\n |-- JOB_ID: string (nullable = true)\n |-- SALARY: integer (nullable = true)\n |-- COMMISSION_PCT: string (nullable = true)\n |-- MANAGER_ID: integer (nullable = true)\n |-- DEPARTMENT_ID: integer (nullable = true)\n\n+-----------+----------+---------+--------+------------+----------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER| HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+----------+----------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|2007-06-21|  SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|2008-01-13|  SH_CLERK|  2600|            - |       124|           50|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|2003-09-17|   AD_ASST|  4400|            - |       101|           10|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|2004-02-17|    MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|2005-08-17|    MK_REP|  6000|            - |       201|           20|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|2002-06-07|    HR_REP|  6500|            - |       101|           40|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|2002-06-07|    PR_REP| 10000|            - |       101|           70|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|2002-06-07|    AC_MGR| 12008|            - |       101|          110|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|2002-06-07|AC_ACCOUNT|  8300|            - |       205|          110|\n|        100|    Steven|     King|   SKING|515.123.4567|2003-06-17|   AD_PRES| 24000|            - |      null|           90|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2005-09-21|     AD_VP| 17000|            - |       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2001-01-13|     AD_VP| 17000|            - |       100|           90|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|2006-01-03|   IT_PROG|  9000|            - |       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|2007-05-21|   IT_PROG|  6000|            - |       103|           60|\n|        105|     David|   Austin| DAUSTIN|590.423.4569|2005-06-25|   IT_PROG|  4800|            - |       103|           60|\n|        106|     Valli|Pataballa|VPATABAL|590.423.4560|2006-02-05|   IT_PROG|  4800|            - |       103|           60|\n|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|2007-02-07|   IT_PROG|  4200|            - |       103|           60|\n|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|2002-08-17|    FI_MGR| 12008|            - |       101|          100|\n|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|2002-08-16|FI_ACCOUNT|  9000|            - |       108|          100|\n|        110|      John|     Chen|   JCHEN|515.124.4269|2005-09-28|FI_ACCOUNT|  8200|            - |       108|          100|\n+-----------+----------+---------+--------+------------+----------+----------+------+--------------+----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# How you can typecast or change datatypes of your coloumn\n",
    "\n",
    "df = df.withColumn(\"HIRE_DATE\", to_date(col(\"HIRE_DATE\"), \"dd-MMM-yy\"))  #MMM significes that month is in abbrevated name JUN like that\n",
    "df = df.withColumn(\"MANAGER_ID\", col(\"MANAGER_ID\").cast(IntegerType()))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d229b94-6ff8-449c-a2c1-439243fea720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+--------+------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER| HIRE_DATE|  JOB_ID|SALARY|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+----------+--------+------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|2007-06-21|SH_CLERK|  2600|       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|2008-01-13|SH_CLERK|  2600|       124|           50|\n+-----------+----------+---------+--------+------------+----------+--------+------+----------+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# How to drop coloumn \n",
    "\n",
    "df = df.drop(\"COMMISSION_PCT\") #if multiple use ,\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ae7680-d194-4e24-814f-7d3c972bd497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|EMPLOYEE_ID|FIRST_NAME|\n+-----------+----------+\n|        198|    Donald|\n|        199|   Douglas|\n+-----------+----------+\nonly showing top 2 rows\n\n+-----------------+\n|(EMPLOYEE_ID + 2)|\n+-----------------+\n|              200|\n|              201|\n+-----------------+\nonly showing top 2 rows\n\n+---+\n|EMP|\n+---+\n|200|\n|201|\n+---+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns\n",
    "\n",
    "df.select(\"EMPLOYEE_ID\",\"FIRST_NAME\").show(2) \n",
    "\n",
    "# if want to do operation\n",
    "# Either use withColumn and create a new column or update existing which is recmommened or temporarily use col and add subtract\n",
    "\n",
    "# For temporary purpose \n",
    "df.select(col(\"EMPLOYEE_ID\")+2).show(2)\n",
    "\n",
    "# using expression in select what this does is convert string type things in select as a job  expression is used for complex transformation which you can do directly with sql\n",
    "# any select always expects column only Expr will convert the rest sql into logic\n",
    "\n",
    "df.selectExpr(\"EMPLOYEE_ID + 2 as EMP\").show(2)  # you can write query like logic with Expr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796bfc20-e4ee-41f4-8384-05c8186f8a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|EMP|\n+---+\n|198|\n+---+\nonly showing top 1 row\n\n+-----------+----------+---------+--------+------------+----------+------+------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER| HIRE_DATE|JOB_ID|SALARY|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+----------+------+------+----------+-------------+\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2005-09-21| AD_VP| 17000|       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2001-01-13| AD_VP| 17000|       100|           90|\n+-----------+----------+---------+--------+------------+----------+------+------+----------+-------------+\n\n+-----------+----------+---------+--------+------------+----------+------+------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER| HIRE_DATE|JOB_ID|SALARY|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+----------+------+------+----------+-------------+\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2005-09-21| AD_VP| 17000|       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2001-01-13| AD_VP| 17000|       100|           90|\n+-----------+----------+---------+--------+------------+----------+------+------+----------+-------------+\n\n+-----------+----------+---------+--------+------------+----------+--------+------+----------+-------------+-------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER| HIRE_DATE|  JOB_ID|SALARY|MANAGER_ID|DEPARTMENT_ID|new_col|\n+-----------+----------+---------+--------+------------+----------+--------+------+----------+-------------+-------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|2007-06-21|SH_CLERK|  2600|       124|           50|    lol|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|2008-01-13|SH_CLERK|  2600|       124|           50|    lol|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|2003-09-17| AD_ASST|  4400|       101|           10|    lol|\n+-----------+----------+---------+--------+------------+----------+--------+------+----------+-------------+-------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# How alias work\n",
    "df.select(col(\"EMPLOYEE_ID\").alias(\"EMP\")).show(1)\n",
    "\n",
    "\n",
    "# Filter how to use filter or where both used for same purpose, these also like select will expect coloumn and the logic\n",
    "df.filter((col(\"SALARY\")>13000) & (col(\"MANAGER_ID\")==100)).show()\n",
    "df.where((col(\"SALARY\")>13000) & (col(\"MANAGER_ID\")==100)).show()\n",
    "# when using $ for and , | for or, you have to enclose your columns in brackets like above \n",
    "\n",
    "\n",
    "#what is literal that is lit\n",
    "# Used when we want to create a coloumn with constant value throughtout the coloumn \n",
    "\n",
    "df.select(\"*\", lit(\"lol\").alias(\"new_col\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59582bf-6625-43cd-b1fd-7a132a99ec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|EMPLOYEE_ID|new_emp_is|\n+-----------+----------+\n|        198|       200|\n|        199|       201|\n+-----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# withColumn \n",
    "# This is used when we want to add a newcoloumn or make changes in existing coloumn like type cast etc\n",
    "# withColoumnRenamed is used to rename column\n",
    "\n",
    "df.withColumn(\"new_emp_is\", col(\"EMPLOYEE_ID\")+2)\\\n",
    "    .select(\"EMPLOYEE_ID\",\"new_emp_is\").show(2)\n",
    "# used withcolumn with select can do complex stuff like this also can use if else and case statements will be shown ahead in future slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e131e0da-fbab-42f7-b84c-648e3958c2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----+------------+----------+-------+------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|EMAIL|PHONE_NUMBER| HIRE_DATE| JOB_ID|SALARY|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+-----+------------+----------+-------+------+----------+-------------+\n|        100|    Steven|     King|SKING|515.123.4567|2003-06-17|AD_PRES| 24000|      null|           90|\n+-----------+----------+---------+-----+------------+----------+-------+------+----------+-------------+\n\nOut[65]: DataFrame[EMPLOYEE_ID: int, FIRST_NAME: string, LAST_NAME: string, EMAIL: string, PHONE_NUMBER: string, HIRE_DATE: date, JOB_ID: string, SALARY: int, MANAGER_ID: int, DEPARTMENT_ID: int]"
     ]
    }
   ],
   "source": [
    "# to check for null you can use isNull()\n",
    "\n",
    "df.filter(col(\"MANAGER_ID\").isNull()).show()\n",
    "\n",
    "# to replace you can do fillna\n",
    "\n",
    "df.fillna({\"MANAGER_ID\":-1}) # if multiple column add comma in between key value pairs key is column name value is what you want to set it with \n",
    "\n",
    "# If you want to use dynamic defaults like mean or something you can do it 2 ways \n",
    "\n",
    "# Calculate the mean value\n",
    "mean_value = df.select(mean(col(\"SALARY\"))).collect()[0][0]\n",
    "\n",
    "# Replace nulls with the mean using withColumn\n",
    "df.withColumn(\n",
    "    \"SALARY\",\n",
    "    when(col(\"SALARY\").isNull(), mean_value).otherwise(col(\"SALARY\"))\n",
    ")\n",
    "#use withcolumn for complex process or also instead of null if you want to replace \n",
    "# or \n",
    "\n",
    "df.fillna({\"SALARY\": mean_value})\n",
    "\n",
    "# Complex operation looks like\n",
    "\n",
    "df_replaced = df.replace({\"pending\": \"in_progress\", \"done\": \"completed\"}, subset=[\"status\"])\n",
    "#subset is in which column to replace\n",
    "\n",
    "# if want to replace substitute null and everything you can do it together with withColumn whenotherwise\n",
    "\n",
    "df_transformed = df.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"status\").isNull(), \"Unknown\")            # Replace null with \"Unknown\"\n",
    "    .when(col(\"status\") == \"pending\", \"in_progress\")  # Replace \"pending\" â†’ \"in_progress\"\n",
    "    .otherwise(col(\"status\"))                         # Keep everything else the same\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b0eb61-6dbb-42f8-a98d-b9ee4b3addea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_5(basic_commands)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
