{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e22626c-bb43-4e31-8ac1-8c303f279f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n|    United States|          Singapore|   25|\n|    United States|            Grenada|   54|\n|       Costa Rica|      United States|  477|\n|          Senegal|      United States|   29|\n|    United States|   Marshall Islands|   44|\n+-----------------+-------------------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferschema\",\"true\")\\\n",
    "            .option(\"mode\",\"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df.show(10)\n",
    "\n",
    "''' \n",
    "\t=> Spark is the session that you create \n",
    "\t\t\n",
    "\t=>Format specifies which format like CSV, Json, JDBC connector etc if not mentioned then default it takes as parquet. It is a optional parameter\n",
    "\t\t\n",
    "\t=>Option has various parameters like (“Header”,”true”), (“InferSchema”,”true”)\n",
    "\t\t\n",
    "\t=>Schema is when schema you want to specifically specify\n",
    "\t\t\n",
    "\t=>Load is when you want to load data\n",
    "\n",
    "MODE : Here in option there is a mode parameter it has various meaning\n",
    "\t\t\ta) FAILFAST: whenever any corrupt data is detected fail immediatly the \n",
    "                        whole process will be stopped  (like if data type is integer and data has null it will not run and throw error for whole dataset)\n",
    "  \n",
    "\t\t\tb) DROPMALFORMED: Will only dropped corrupted records\n",
    "\n",
    "\t\t\tc) PERMISSIVE: If No mode mentioned this is the default one. It subsitutes null whenever corrupted record occurs\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe7a8a6-2be9-4834-92f2-a5cbbb131fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema() #Prints the schem as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0f7b75-7d67-4387-88cb-eec40153bf72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How to create a manual schema if we use inferschem as false in options\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "my_schema = StructType([\n",
    "                                StructField(\"destination_country\",StringType(),True),\n",
    "                                StructField(\"origin_country\",StringType(),True),\n",
    "                                StructField(\"count\",IntegerType(),True)\n",
    "                            ])\n",
    "\n",
    "# StructField(\"Name\", DataType(), nullable(True or False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793ccdb6-2881-4173-b9f0-5a62f351b976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----+\n|destination_country|origin_country|count|\n+-------------------+--------------+-----+\n|      United States|       Romania|    1|\n|      United States|       Ireland|  264|\n|      United States|         India|   69|\n|              Egypt| United States|   24|\n|  Equatorial Guinea| United States|    1|\n+-------------------+--------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"false\")\\\n",
    "            .option(\"skipRows\",1)\\\n",
    "            .option(\"inferschema\",\"false\")\\\n",
    "            .schema(my_schema)\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "            .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "        \n",
    "df.show(5)\n",
    "\n",
    "# This is how you can use your own schema "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_(Read data, Create schemas)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
