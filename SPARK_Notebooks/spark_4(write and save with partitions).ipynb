{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7509a4-db2b-44bd-adca-90993b5bb532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we will see how usually you write data to some place what is the format and different parameters \n",
    "\n",
    "'''\n",
    "basic struct as follows:\n",
    "\n",
    "df.write.format()\\\n",
    "        .option()\\ can be multiple options\n",
    "        .partitionBy()\\\n",
    "        .bucketBy()\\\n",
    "        .save()\\\n",
    "\n",
    "example:\n",
    "    \n",
    "    df.write.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"mode\",\"append\")\\  mode can be overwrite, append, errorIfExist(if file exist will give error), ignore(if fle exist will not write and move forward but will not give error) etc\n",
    "            .option(\"path\",\"---\")\\\n",
    "            .save() you can write path directly inside save if not using path option \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628086ba-436f-40aa-8fb8-97ebff888a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n| id|    name|age|salary|address|gender|\n+---+--------+---+------+-------+------+\n|  1|  Manish| 26| 75000|  INDIA|     m|\n|  2|  Nikita| 23|100000|    USA|     f|\n|  3|  Pritam| 22|150000|  INDIA|     m|\n|  4|Prantosh| 17|200000|  JAPAN|     m|\n|  5|  Vikash| 31|300000|    USA|     m|\n|  6|   Rahul| 55|300000|  INDIA|     m|\n|  7|    Raju| 67|540000|    USA|     m|\n|  8| Praveen| 28| 70000|  JAPAN|     m|\n|  9|     Dev| 32|150000|  JAPAN|     m|\n| 10|  Sherin| 16| 25000| RUSSIA|     f|\n| 11|    Ragu| 12| 35000|  INDIA|     f|\n| 12|   Sweta| 43|200000|  INDIA|     f|\n| 13| Raushan| 48|650000|    USA|     m|\n| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n| 15| Prakash| 52|750000|  INDIA|     m|\n+---+--------+---+------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"/FileStore/tables/spark_4-1.txt\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26cb7b25-a6f8-4c90-a743-e0a99887fa42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write with partitioning and buckting\n",
    "\n",
    "'''\n",
    "questions asked\n",
    "\n",
    "1) what is partitioning and bucketing\n",
    "2) why do we need it \n",
    "3) when to use which\n",
    "\n",
    "basically partition is used when we have a coloumn wherein records are less and very much repetetive so we can use that coloumn and create partition based on the different attributes like if i have a coloumn of days so i can basically create partitions based on days like 7 partitions \n",
    "\n",
    "on the order hand bucketing is used when you dont have such coloumns like there are a lot od distinct characteristic in your coloumns so partition will fail so indirectly you will just use some other condition to store data so bucket is like ki etna mb hogaya tho bas hogaya ye bucket ke liye and some other condition so using one coloumn like id we will bucket ki 1 to 4 in one bucket and 41 to 9 in another like that\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d656d9-8e25-4d20-88dc-4958379c8d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we will use partition and bucket here here to write according to our data \n",
    "\n",
    "\n",
    "df.write.format(\"csv\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"mode\",\"overwrite\")\\\n",
    "        .option(\"path\",\"/FileStore/tables/partitioned_data/\")\\\n",
    "        .partitionBy(\"address\")\\\n",
    "        .save()\n",
    "\n",
    "# you can also use nested partition use coloumn with coma saparated and order it in how you want like you want to have partition of adress and inside it partition it by gender so do address, gender like that in the paramter order is important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17c89eb-6f86-405f-870b-026181241d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"mode\",\"overwrite\")\\\n",
    "        .option(\"path\",\"/FileStore/tables/bucketed_data/\")\\\n",
    "        .bucketBy(3,\"id\")\\\n",
    "        .saveAsTable(\"bucket_data_by_id_table\")\n",
    "# 3 says that create 3 equal buckets it will try to create 3 equal buckets with respect to our id here id will come sorted order\n",
    "\n",
    "# one more thing to note is save() doesnt work/support bucket so u need to use saveastable \n",
    "\n",
    "# Important: There is a problem in bucketBy that if there are 200 task running simultaneously bucketby will create new buckets for each task so if we specify 5 buckets and 200 tasks are running it will create 200 X 5 buckets which we dont want so in order to avoid that we use repartition first then apply bucketby so what it does is it partitions your data into 5 partitions howmuchever task there may be and upon that you can apply your bucket\n",
    "\n",
    "# where will you use bucketBy then so think of adhar info you will not get a coloumn which will have less atrributes so at that time we use bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f0ebf8-9a19-4db7-a4fd-c600b1b53a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n| id|    name|age|salary|address|gender|\n+---+--------+---+------+-------+------+\n|  2|  Nikita| 23|100000|    USA|     f|\n|  3|  Pritam| 22|150000|  INDIA|     m|\n|  4|Prantosh| 17|200000|  JAPAN|     m|\n|  5|  Vikash| 31|300000|    USA|     m|\n|  7|    Raju| 67|540000|    USA|     m|\n|  8| Praveen| 28| 70000|  JAPAN|     m|\n|  9|     Dev| 32|150000|  JAPAN|     m|\n| 10|  Sherin| 16| 25000| RUSSIA|     f|\n| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n+---+--------+---+------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"/FileStore/tables/bucketed_data/part-00000-tid-5871968376594103200-6182c160-470c-4f57-bf68-2137aa40ff21-7-1_00000.c000.csv\")\n",
    "\n",
    "df1.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_4(write and save with partitions)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
