{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b6dbc2-6440-4ec7-a348-a5c1899c8d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Aggregation like count,sum,avg,min,max\n",
    "\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "schema = ['id','name','age','sal','country','dept']\n",
    "\n",
    "df = spark.createDataFrame(data = emp_data, schema = schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de60f3e-5673-4121-9fd5-ea9dd9b5a06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# counts start\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8543fd-9e7f-463c-9ba3-66e6275c7cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 10"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b57181-7a7a-41d2-b65b-1273c366a082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"name\")).show()\n",
    "# this showed you the value of all names but if there is null it will ignore that when you use count with a specific column it does this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecf7244-ffac-44bc-9d40-19ea1dc626cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-------------------+\n|salary_sum|salary_max|salary_min|Average_without_avg|\n+----------+----------+----------+-------------------+\n|    560000|    200000|     20000|              70000|\n+----------+----------+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# one code to show all like example\n",
    "\n",
    "df.select(sum('sal').alias(\"salary_sum\"), max('sal').alias(\"salary_max\"), min('sal').alias(\"salary_min\"), (sum('sal')/count('sal')).cast(IntegerType()).alias(\"Average_without_avg\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1f4e68-b0ff-4e43-bf9a-ff66b88d4313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group BY \n",
    "\n",
    "data = [(1,'manish',50000,\"IT\"),\n",
    "(2,'vikash',60000,\"sales\"),\n",
    "(3,'raushan',70000,\"marketing\"),\n",
    "(4,'mukesh',80000,\"IT\"),\n",
    "(5,'pritam',90000,\"sales\"),\n",
    "(6,'nikita',45000,\"marketing\"),\n",
    "(7,'ragini',55000,\"marketing\"),\n",
    "(8,'rakesh',100000,\"IT\"),\n",
    "(9,'aditya',65000,\"IT\"),\n",
    "(10,'rahul',50000,\"marketing\")]\n",
    "\n",
    "schema1 = ['id','name','sal','dept']\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema = schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a9cc81-de52-491b-a14f-2b04a6826f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|     dept|sum(sal)|\n+---------+--------+\n|       IT|  295000|\n|    sales|  150000|\n|marketing|  220000|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"dept\").agg(sum('sal')).show()\n",
    "# can do multiple agg with , in between as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34185665-df7e-43fa-9dfa-4cc4c53a7340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.groupBy(\"dept\").agg(sum('sal').alias(\"total_sal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4aba35-5912-47d7-982e-f500b5f76b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+---------+---------+\n| id|   name|   sal|     dept|     dept|total_sal|\n+---+-------+------+---------+---------+---------+\n|  1| manish| 50000|       IT|       IT|   295000|\n|  2| vikash| 60000|    sales|    sales|   150000|\n|  3|raushan| 70000|marketing|marketing|   220000|\n|  5| pritam| 90000|    sales|    sales|   150000|\n|  4| mukesh| 80000|       IT|       IT|   295000|\n|  6| nikita| 45000|marketing|marketing|   220000|\n|  7| ragini| 55000|marketing|marketing|   220000|\n|  8| rakesh|100000|       IT|       IT|   295000|\n| 10|  rahul| 50000|marketing|marketing|   220000|\n|  9| aditya| 65000|       IT|       IT|   295000|\n+---+-------+------+---------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# How can we use join \n",
    "\n",
    "# lets take the above data only \n",
    "# now our question is output all data along with another column where that column will have total salary of employee of that department only \n",
    "# This is the syntax \"inner\", \"left\", \"right\" are the types that u can use there are others as well\n",
    "\n",
    "df1.join(df2, df1[\"dept\"]==df2[\"dept\"],\"inner\").show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e27159a-7b6e-40e1-bf1a-0fc06b43134c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---------+\n| id|   sal|     dept|total_sal|\n+---+------+---------+---------+\n|  1| 50000|       IT|   295000|\n|  2| 60000|    sales|   150000|\n|  3| 70000|marketing|   220000|\n|  5| 90000|    sales|   150000|\n|  4| 80000|       IT|   295000|\n|  6| 45000|marketing|   220000|\n|  7| 55000|marketing|   220000|\n|  8|100000|       IT|   295000|\n| 10| 50000|marketing|   220000|\n|  9| 65000|       IT|   295000|\n+---+------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.alias(\"a\").join(df2.alias(\"b\"), col(\"a.dept\") == col(\"b.dept\"), \"inner\") \\\n",
    "    .select(col(\"a.id\"), col(\"a.sal\"), col(\"a.dept\"), col(\"b.total_sal\")) \\\n",
    "    .show()\n",
    "\n",
    "'''\n",
    "if multiple column based join then use & and enclose each condition with ()\n",
    "\n",
    "df1.alias(\"a\").join(df2.alias(\"b\"), ( col(\"a.dept\") == col(\"b.dept\") ) & ( col(\"a.something\") == col(\"b.something\") ), \"inner\") \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5898cc64-e2cf-4b9b-810a-9f5d4cb64fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: '\\nGeneral structure first we need to import window\\n\\nfrom pyspark.sql.window import Window\\n\\nthen create a window and keep basically u are defining and keeping partionby and order by \\n\\nwindow = Window.partitionBy().orderBy()\\n\\nwhen i want to use it \\n\\ndf,withColumn(\"new\", dense_rank().over(window))\\n'"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "\n",
    "'''\n",
    "General structure first we need to import window\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "then create a window and keep basically u are defining and keeping partionby and order by \n",
    "\n",
    "window = Window.partitionBy().orderBy()\n",
    "\n",
    "when i want to use it \n",
    "\n",
    "df,withColumn(\"new\", dense_rank().over(window))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62880af1-bc78-42b9-aef7-6b0708d13ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+---------+\n| id|   name|   sal|     dept|total_sal|\n+---+-------+------+---------+---------+\n|  8| rakesh|100000|       IT|   295000|\n|  4| mukesh| 80000|       IT|   295000|\n|  9| aditya| 65000|       IT|   295000|\n|  1| manish| 50000|       IT|   295000|\n|  3|raushan| 70000|marketing|   220000|\n|  7| ragini| 55000|marketing|   220000|\n| 10|  rahul| 50000|marketing|   220000|\n|  6| nikita| 45000|marketing|   220000|\n|  5| pritam| 90000|    sales|   150000|\n|  2| vikash| 60000|    sales|   150000|\n+---+-------+------+---------+---------+\n\n+---+-------+------+---------+---------+\n| id|   name|   sal|     dept|total_sal|\n+---+-------+------+---------+---------+\n|  8| rakesh|100000|       IT|   100000|\n|  4| mukesh| 80000|       IT|   180000|\n|  9| aditya| 65000|       IT|   245000|\n|  1| manish| 50000|       IT|   295000|\n|  3|raushan| 70000|marketing|    70000|\n|  7| ragini| 55000|marketing|   125000|\n| 10|  rahul| 50000|marketing|   175000|\n|  6| nikita| 45000|marketing|   220000|\n|  5| pritam| 90000|    sales|    90000|\n|  2| vikash| 60000|    sales|   150000|\n+---+-------+------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Lets take the question from join one wherein we had to find total salary in another column if i had to do that with window the approach is below\n",
    "window1 = Window.partitionBy('dept')\n",
    "df1.withColumn(\"total_sal\", sum('sal').over(window1))\\\n",
    "    .sort('dept',desc('sal')).show()\n",
    "\n",
    "# if i want running total\n",
    "window = Window.partitionBy('dept').orderBy(desc('sal'))\n",
    "df1.withColumn(\"total_sal\", sum('sal').over(window)).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8470606-ca6e-4f61-8805-489c979f1d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[47]: \"Reffer This video for the same to understand it better since its lengthy to write \\n\\n        https://www.youtube.com/watch?v=FNrOnHNzoJI&list=PLTsNSGeIpGnGjaMSYVlidqVWSjKWoBhbr&index=23\\n\\n        window = Window.partitionBy('dept').orderBy(desc('sal')).rowsBetween()\\n        OR\\n        window = Window.partitionBy('dept').orderBy(desc('sal')).rangeBetween() \\n\\n\\n        rows between and range between takes 2 paramter\\n\\n        parameters could be 0=> current row\\n                            -ve number => previous rows\\n                            +ve number => rows ahead of current\\n                            unbounded preceding => all rows above current row\\n                            unbounded => following all rows below current row\\n\\n        so it can be like \\n\\n       rangeBetween(-6, 0):\\n// Includes rows where the column value is between (current_row_value - 6) and current_row_value.\\nlike if i want data for last 7 days then it is -6,0 i.e 0 is the 1st day and previous 6 days so -6 total 7 days\\n\\nrangeBetween(0, 6):\\n// Includes rows where the column value is between current_row_value and (current_row_value + 6) records.\\n\""
     ]
    }
   ],
   "source": [
    "# Also there also one concept where you can find running total between a range of days like suppose range of 3 days or something\n",
    "\n",
    "# Rows between and range between\n",
    "\n",
    "'''Reffer This video for the same to understand it better since its lengthy to write \n",
    "\n",
    "        https://www.youtube.com/watch?v=FNrOnHNzoJI&list=PLTsNSGeIpGnGjaMSYVlidqVWSjKWoBhbr&index=23\n",
    "\n",
    "        window = Window.partitionBy('dept').orderBy(desc('sal')).rowsBetween()\n",
    "        OR\n",
    "        window = Window.partitionBy('dept').orderBy(desc('sal')).rangeBetween() \n",
    "\n",
    "\n",
    "        rows between and range between takes 2 paramter\n",
    "\n",
    "        parameters could be 0=> current row\n",
    "                            -ve number => previous rows\n",
    "                            +ve number => rows ahead of current\n",
    "                            unbounded preceding => all rows above current row\n",
    "                            unbounded => following all rows below current row\n",
    "\n",
    "        so it can be like \n",
    "\n",
    "       rangeBetween(-6, 0):\n",
    "// Includes rows where the column value is between (current_row_value - 6) and current_row_value.\n",
    "like if i want data for last 7 days then it is -6,0 i.e 0 is the 1st day and previous 6 days so -6 total 7 days\n",
    "\n",
    "rangeBetween(0, 6):\n",
    "// Includes rows where the column value is between current_row_value and (current_row_value + 6) records.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ee2164-b723-4ce7-8e03-3e4ce1d5a0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_7(agg, group, joins,window f)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
