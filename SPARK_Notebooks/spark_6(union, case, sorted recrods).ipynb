{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded39032-9921-4f24-a5ca-15eee3332010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 11| Vikas|75000|     16|\n| 12| Nisha|40000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|80000|     18|\n| 15| Mohit|45000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n+---+------+-----+-------+\n\n+---+--------+-----+-------+\n| id|emp_name|  sal|mngr_id|\n+---+--------+-----+-------+\n| 19|   Sohan|50000|     18|\n| 20|    Sima|75000|     17|\n| 18|     Sam|65000|     17|\n+---+--------+-----+-------+\n\n+--------+---+-----+-------+\n|emp_name| id|  sal|mngr_id|\n+--------+---+-----+-------+\n|   sohan| 19|50000|     18|\n|    sima| 20|75000|     17|\n+--------+---+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Data Creation\n",
    "\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema = [\"id\",\"name\",\"sal\",\"mngr_id\"]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "schema1 = [\"id\",\"emp_name\",\"sal\",\"mngr_id\"]\n",
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "\n",
    "df1.show()\n",
    "\n",
    "data_diff = [('sohan',19 ,50000, 18),\n",
    "('sima',20 ,75000,  17)]\n",
    "\n",
    "schema_diff = [\"emp_name\",\"id\",\"sal\",\"mngr_id\"]\n",
    "\n",
    "diff = spark.createDataFrame(data=data_diff,schema=schema_diff)\n",
    "diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15c96c8-9340-40da-9e55-3f571e1d6541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 11| Vikas|75000|     16|\n| 12| Nisha|40000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|80000|     18|\n| 15| Mohit|45000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 19| Sohan|50000|     18|\n| 20|  Sima|75000|     17|\n| 18|   Sam|65000|     17|\n+---+------+-----+-------+\n\n+---+------+-----+-------+\n| id|  name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 11| Vikas|75000|     16|\n| 12| Nisha|40000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|80000|     18|\n| 15| Mohit|45000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 19| Sohan|50000|     18|\n| 20|  Sima|75000|     17|\n| 18|   Sam|65000|     17|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.union(df1).show() #Column name will be used of the coloums of which u are using as start here in this case df\n",
    "df.unionAll(df1).show() #unionAll\n",
    "\n",
    "'''\n",
    "With respect to dataframe union and unionAll will give you same result that is duplicates will be inlcuded as well it will take both the table and add all rows together \n",
    "\n",
    "on the other hand if you use sql or spark sql then when using union the output will give you table and will not consider duplicate and unionAll will also take duplicate \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fc6807-bb5f-4440-b2f0-86bf9b9974ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+-------+\n|emp_name|   id|  sal|mngr_id|\n+--------+-----+-----+-------+\n|   sohan|   19|50000|     18|\n|    sima|   20|75000|     17|\n|      19|Sohan|50000|     18|\n|      20| Sima|75000|     17|\n|      18|  Sam|65000|     17|\n+--------+-----+-----+-------+\n\n+--------+---+-----+-------+\n|emp_name| id|  sal|mngr_id|\n+--------+---+-----+-------+\n|   sohan| 19|50000|     18|\n|    sima| 20|75000|     17|\n|   Sohan| 19|50000|     18|\n|    Sima| 20|75000|     17|\n|     Sam| 18|65000|     17|\n+--------+---+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# If column order is not same it will still do union but data will be a mess\n",
    "diff.union(df1).show() \n",
    "\n",
    "# To avoid this we do unionByName what this will do is while doing union spark will find column name match it and put data below it \n",
    "# We can also use select statement and select in particular order then use union (alternative)\n",
    "\n",
    "#IMP unionByName should only be used when you know the column names will be same, if not same and you want to do proper union then use select and create a proper order between the 2 dataframes and then use union so this will give you result then \n",
    "# How it would look like: df1.select(\"columns with same order\").union(df2.select(\"columns with same order\")) here columns name could be different but order if mainted same then union will work correctly \n",
    "\n",
    "diff.unionByName(df1).show()\n",
    "\n",
    "# ALways in all union column names are used of first dataframe that you use in code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc570a57-1382-4b37-9c76-d430418f76ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we will learn about case when situations \n",
    "\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "schema = ['id','name','age','salary','country','dept']\n",
    "df = spark.createDataFrame(data=emp_data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15cccd22-c637-40a0-a8f4-e21840893038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pySpark distinct() transformation is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns. distinct() and dropDuplicates() returns a new DataFrame. In this article, you will learn how to use distinct() and dropDuplicates() functions with PySpark example.\n",
    "\n",
    "\n",
    "df = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cdb180b-fbb8-4136-8c51-51ed5ce62927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''if you want to drop any row in which any value is null, use\n",
    "\n",
    "df.na.drop()  //same as df.na.drop(\"any\") default is \"any\"\n",
    "or\n",
    "df.na.drop(subset=['columns']) //for specific columns\n",
    "to drop only if all values are null for that row, use\n",
    "df.na.drop(\"all\")\n",
    "\n",
    "'''\n",
    "\n",
    "df = df.na.drop('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf07b29-d01a-4b75-8f4a-845a975e7cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+-------+-----------+\n| id|   name| age|salary|country|       dept|\n+---+-------+----+------+-------+-----------+\n|  1| manish|  26| 20000|  india|         IT|\n|  2|  rahul|null| 40000|germany|engineering|\n|  3|  pawan|  12| 60000|  india|      sales|\n|  4|roshini|  44|  null|     uk|engineering|\n|  5|raushan|  35| 70000|  india|      sales|\n|  6|   null|  29|200000|     uk|         IT|\n|  7|   adam|  37| 65000|     us|         IT|\n|  8|  chris|  16| 40000|     us|      sales|\n+---+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ec7dd3-8295-4810-8e86-45beff19af9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+-------+-----------+-------------+\n| id|   name| age|salary|country|       dept|        adult|\n+---+-------+----+------+-------+-----------+-------------+\n|  1| manish|  26| 20000|  india|         IT|          Yes|\n|  2|  rahul|null| 40000|germany|engineering|Not Mentioned|\n|  3|  pawan|  12| 60000|  india|      sales|           No|\n|  4|roshini|  44|  null|     uk|engineering|          Yes|\n|  5|raushan|  35| 70000|  india|      sales|          Yes|\n|  6|   null|  29|200000|     uk|         IT|          Yes|\n|  7|   adam|  37| 65000|     us|         IT|          Yes|\n|  8|  chris|  16| 40000|     us|      sales|           No|\n+---+-------+----+------+-------+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df.withColumn(\"adult\", when(col('age')>=18,\"Yes\").when(col('age').isNull(),\"Not Mentioned\").otherwise(\"No\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140d42d9-a14d-49c4-b160-eb034801973c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-------+-----------+\n| id|   name|age|salary|country|       dept|\n+---+-------+---+------+-------+-----------+\n|  1| manish| 26| 20000|  india|         IT|\n|  2|  rahul| 20| 40000|germany|engineering|\n|  3|  pawan| 12| 60000|  india|      sales|\n|  4|roshini| 44|  null|     uk|engineering|\n|  5|raushan| 35| 70000|  india|      sales|\n|  6|   null| 29|200000|     uk|         IT|\n|  7|   adam| 37| 65000|     us|         IT|\n|  8|  chris| 16| 40000|     us|      sales|\n+---+-------+---+------+-------+-----------+\n\n+---+-------+---+------+-------+-----------+\n| id|   name|age|salary|country|       dept|\n+---+-------+---+------+-------+-----------+\n|  1| manish| 26| 20000|  india|         IT|\n|  2|  rahul| 20| 40000|germany|engineering|\n|  3|  pawan| 12| 60000|  india|      sales|\n|  4|roshini| 44|  null|     uk|engineering|\n|  5|raushan| 35| 70000|  india|      sales|\n|  6|   null| 29|200000|     uk|         IT|\n|  7|   adam| 37| 65000|     us|         IT|\n|  8|  chris| 16| 40000|     us|      sales|\n+---+-------+---+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# to set a default value to null for a particular column you have replace na function but if want to do with when then\n",
    "\n",
    "df.withColumn(\"age\", when(col(\"age\").isNull(),20).otherwise(col(\"age\"))).show()\n",
    "\n",
    "#althernative\n",
    "\n",
    "df.fillna(value=20, subset=['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3957c40-c1fd-4918-bae2-2da861996d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if you want to use and in when case you have to enclose your col\n",
    "# example: df.withColumn(\"column\", when( (col('age')>0) & (col('age')<18) ,minor ) ) \n",
    "# like this both col should be enclose only then will and or conditions will work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3761619b-bf0b-48b6-9c31-5ce58f722067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+-------+-----------+\n| id|   name| age|salary|country|       dept|\n+---+-------+----+------+-------+-----------+\n|  4|roshini|  44|  null|     uk|engineering|\n|  1| manish|  26| 20000|  india|         IT|\n|  2|  rahul|null| 40000|germany|engineering|\n|  8|  chris|  16| 40000|     us|      sales|\n|  3|  pawan|  12| 60000|  india|      sales|\n|  7|   adam|  37| 65000|     us|         IT|\n|  5|raushan|  35| 70000|  india|      sales|\n|  6|   null|  29|200000|     uk|         IT|\n+---+-------+----+------+-------+-----------+\n\n+---+-------+----+------+-------+-----------+\n| id|   name| age|salary|country|       dept|\n+---+-------+----+------+-------+-----------+\n|  6|   null|  29|200000|     uk|         IT|\n|  5|raushan|  35| 70000|  india|      sales|\n|  7|   adam|  37| 65000|     us|         IT|\n|  3|  pawan|  12| 60000|  india|      sales|\n|  2|  rahul|null| 40000|germany|engineering|\n|  8|  chris|  16| 40000|     us|      sales|\n|  1| manish|  26| 20000|  india|         IT|\n|  4|roshini|  44|  null|     uk|engineering|\n+---+-------+----+------+-------+-----------+\n\n+---+-------+----+------+-------+-----------+\n| id|   name| age|salary|country|       dept|\n+---+-------+----+------+-------+-----------+\n|  6|   null|  29|200000|     uk|         IT|\n|  5|raushan|  35| 70000|  india|      sales|\n|  7|   adam|  37| 65000|     us|         IT|\n|  3|  pawan|  12| 60000|  india|      sales|\n|  2|  rahul|null| 40000|germany|engineering|\n|  8|  chris|  16| 40000|     us|      sales|\n|  1| manish|  26| 20000|  india|         IT|\n|  4|roshini|  44|  null|     uk|engineering|\n+---+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#sorting data in spark\n",
    "# By default its ascending\n",
    "df.sort(col(\"salary\")).show()\n",
    "# if want desecnding\n",
    "df.sort(col(\"salary\").desc()).show()\n",
    "\"OR\"\n",
    "df.sort(desc(\"salary\")).show()\n",
    "# can use multiple columns with , as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38aaf019-894d-493d-b68f-1f230a6190ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_6(union, case, sorted recrods)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
